{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "3F8R00dSR-d1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, lag, avg, when\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
        "from pyspark.ml.regression import LinearRegression, GBTRegressor\n",
        "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
        "from pyspark.ml.evaluation import RegressionEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"TimeSeriesPrediction\").getOrCreate()"
      ],
      "metadata": {
        "id": "j9UV-J5gTBHQ"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = spark.read.csv(\"/content/HDEF.csv\", header=True, inferSchema=True)\n",
        "data = data.withColumn(\"Date\", col(\"Date\").cast(\"timestamp\"))\n"
      ],
      "metadata": {
        "id": "s-p7M0fySKmF"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-RWoUpMXHg8",
        "outputId": "960edafb-fb92-4ff2-df8b-243b02129ce3"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, lag, avg\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "windowSpec = Window.orderBy(\"Date\")\n",
        "\n",
        "data = data.withColumn(\"Prev_Close\", lag(\"Close\", 1).over(windowSpec))\n",
        "data = data.withColumn(\"MA_7\", avg(\"Close\").over(windowSpec.rowsBetween(-6, 0)))\n",
        "\n",
        "data = data.dropna()  # Drop rows with null values\n"
      ],
      "metadata": {
        "id": "LvlTV6k3WmD8"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "feature_cols = [\"Open\", \"High\", \"Low\", \"Prev_Close\", \"MA_7\", \"Volume\"]\n",
        "existing_cols = [col_name for col_name in feature_cols if col_name in data.columns]\n",
        "\n",
        "assembler = VectorAssembler(inputCols=existing_cols, outputCol=\"features\")\n",
        "data1 = assembler.transform(data).select(\"features\", \"Close\")\n"
      ],
      "metadata": {
        "id": "kIhYZqJ4Wo5u"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUA2xsyrXw0M",
        "outputId": "774dc49f-05d6-423d-d0e6-bad2f62b15b0"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'Prev_Close', 'MA_7']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train, test = data1.randomSplit([0.8, 0.2], seed=42)\n"
      ],
      "metadata": {
        "id": "qAe2sXtgWq9l"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "\n",
        "lr = LinearRegression(featuresCol=\"features\", labelCol=\"Close\")\n",
        "lr_model = lr.fit(train)\n",
        "reg_predictions = lr_model.transform(test)\n"
      ],
      "metadata": {
        "id": "heLvYoWZWtNN"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "evaluator = RegressionEvaluator(labelCol=\"Close\", metricName=\"rmse\")\n",
        "rmse = evaluator.evaluate(reg_predictions)\n",
        "print(f\"Regression RMSE: {rmse}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXjHq5s0Wwr0",
        "outputId": "fd0b0424-f3d5-42ef-ac47-0b0bf4b1af76"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Regression RMSE: 0.05823250961918156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when\n",
        "\n",
        "data = data.withColumn(\"Price_Change\", when(col(\"Close\") > col(\"Prev_Close\"), \"Up\").otherwise(\"Down\"))\n"
      ],
      "metadata": {
        "id": "EGZ2HehRW0vc"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "indexer = StringIndexer(inputCol=\"Price_Change\", outputCol=\"label\")\n",
        "data2 = indexer.fit(data).transform(data)\n"
      ],
      "metadata": {
        "id": "4ZmezdK5X7zD"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_cols = [\"Open\", \"High\", \"Low\", \"Prev_Close\", \"MA_7\", \"Volume\"]\n",
        "existing_cols = [col_name for col_name in feature_cols if col_name in data2.columns]\n",
        "assembler = VectorAssembler(inputCols=existing_cols, outputCol=\"features\")\n",
        "data3 = assembler.transform(data2)"
      ],
      "metadata": {
        "id": "eSK9RR6-Zgl1"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data3.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pqc-fdvxYCEc",
        "outputId": "0a7d7360-af44-404b-c42c-0508ce2ee6be"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'Prev_Close', 'MA_7', 'Price_Change', 'label', 'features']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train, test = data3.randomSplit([0.8, 0.2], seed=42)"
      ],
      "metadata": {
        "id": "IfhSI-I5YFl0"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YreszxyrYNru",
        "outputId": "a6a9f322-4834-4612-b4f4-4a5d69df8c8c"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[Date: timestamp, Open: double, High: double, Low: double, Close: double, Adj Close: double, Volume: int, Prev_Close: double, MA_7: double, Price_Change: string, label: double, features: vector]"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "\n",
        "# Define hyperparameter grid\n",
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(lr.regParam, [0.1, 0.01, 0.001]) \\\n",
        "    .build()\n",
        "\n",
        "# Cross-validation\n",
        "crossval = CrossValidator(\n",
        "    estimator=lr,\n",
        "    estimatorParamMaps=paramGrid,\n",
        "    evaluator=evaluator,\n",
        "    numFolds=3\n",
        ")\n",
        "\n",
        "# Fit cross-validated model\n",
        "cvModel = crossval.fit(train)\n",
        "\n",
        "# Evaluate on test set\n",
        "cv_predictions = cvModel.transform(test)\n",
        "rmse = evaluator.evaluate(cv_predictions)\n",
        "print(f\"Optimized Regression RMSE: {rmse}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyaP3Hy_cOnO",
        "outputId": "a8a2d84b-bfb9-4738-8a58-62b10a724779"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimized Regression RMSE: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=100)\n",
        "rf_model = rf.fit(train)\n",
        "class_predictions = rf_model.transform(test)"
      ],
      "metadata": {
        "id": "0HRpDQ8NYI88"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_model.save(\"rf_model\")\n",
        "lr_model.save(\"lr_model\")\n"
      ],
      "metadata": {
        "id": "0w6I39Lncfke"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import RandomForestClassificationModel\n",
        "loaded_model = RandomForestClassificationModel.load(\"rf_model\")\n"
      ],
      "metadata": {
        "id": "aHK8-uVlci58"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder \\\n",
        "    .appName(\"BigDataML\") \\\n",
        "    .config(\"spark.executor.memory\", \"4g\") \\\n",
        "    .config(\"spark.driver.memory\", \"2g\") \\\n",
        "    .config(\"spark.executor.cores\", \"4\") \\\n",
        "    .getOrCreate()\n"
      ],
      "metadata": {
        "id": "6gf8Nedtcl49"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(class_predictions)\n",
        "print(f\"Classification Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHYG0P8jZvzE",
        "outputId": "6640c7cc-2fdf-4705-9fdd-3827f205c1ac"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Accuracy: 0.8292682926829268\n"
          ]
        }
      ]
    }
  ]
}